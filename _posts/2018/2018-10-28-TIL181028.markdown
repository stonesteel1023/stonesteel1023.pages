---
layout : post
date : 2018-10-28 18:30
title : "To 30 Billion and Beyond"
comments: true
---

# To 30 Billion and Beyond

> 출처: https://www.haruair.com/blog/4108

* Several years ago I created a free web service, ipify. ipify is a freely available, highly scalable ip address lookup service. When you query its REST API, it returns your public-facing IP address.

I created ipify because at the time I was building complex infrastructure management software and needed to dynamically discover the public IP address of some cloud instances without using any management APIs.

When I searched online for freely available reverse IP lookup services I didn’t find any suitable solutions:

There were websites I could attempt to scrape my IP from (but this is bad form, and would likely result in complaints from the host)
There were APIs for this that charged money (yuck!)
There were APIs that allowed you to do a limited number of lookups per day (which scared me as I was managing a lot of instances at the time)
There were APIs that appeared to be what I wanted, but upon using them they’d error out, go down randomly, or just otherwise not be of high quality. When I inspected the dig records of a particular provider, I noticed that the entire service was running on a single server (with an A record) that was terminating requests directly: not the most scalable/highly available service in the world.
There was an API service that looked somewhat OK but was trying to raise money via donations to stay alive. Integrating with an API service on the brink of death didn’t make me feel terribly comfortable.
Because of all these reasons I figured I’d just throw a small service together myself and solve the problem for as many people as possible. After all: writing software to return a single string isn’t terribly hard: why shouldn’t I do it?

I assumed that worst case I’d spend $30 bucks or so per month and treat it as a public service.

* 몇 년 전에 무료 웹서비스인 ipify를 만들었습니다. ipify는 무료로 사용할 수 있는, 고도로 확장 가능한 IP 주소 검색 서비스입니다. REST API를 호출하면 어느 퍼블릭 IP 주소를 사용하고 있는지 확인 할 수 있습니다.

ipify를 만든 당시에는 복잡한 인프라스트럭처 관리 소프트웨어를 만들고 있었고 클라우드 인스턴스에서 관리용 API를 사용하지 않고 동적으로 퍼블릭 IP 주소를 알아내야 했었습니다.

무료로 사용 가능한 역 IP 검색 서비스를 검색해봤지만 적합한 해결책을 찾지 못했습니다.

IP를 스크랩 할 수 있는 웹사이트가 있긴 했습니다. (하지만 별로 좋은 형태가 아니고 호스트에서 사용하기엔 불편합니다)
비용을 청구하는 API도 있었습니다. (잌!)
일간 조회 횟수를 제한하는 API도 있었습니다. (한꺼번에 많은 인스턴스를 관리해야 했기에 사용하기 두려웠습니다)
제가 원하는 방식의 API도 있었지만, 오류가 발생하기도 하고 가끔 서비스가 내려가기도 하거나 질적으로 떨어지기도 했습니다. 어느 제공자를 dig로 살펴봤더니 전체 서비스가 단일 서버 (A 레코드)에서 동작하고 그 서버에서 요청이 직접 종료되었습니다. 세계 규모로 확장 가능하거나 고가용성의 서비스라고 말하기 어려웠습니다.
보기엔 괜찮은 API도 있었지만, 지속해서 유지하기 위한 기부를 받으려고 했습니다. 사용하려는 API 서비스가 언제 죽을지 모를 상황이라니 마음 놓고 사용할 수 없었습니다.
이런 이유로 인해 작은 서비스를 직접 만들어서 내 문제와 가능한 한 많은 사람의 문제를 해결하고 싶었습니다. 더군다나 하나의 문자열을 반환하는 소프트웨어를 작성하는 일은 그렇게 엄청 어려운 일은 아니었습니다. 하지 말아야 할 이유가 있을까요?

가장 최악의 경우라고 상상해볼 수 있는 일은 기꺼해야 월 30달러가량 사용하고 공개 서비스라는 점을 염두에 두고 다루는 정도였습니다.

## ipify v0

* The first iteration of ipify was quite simple. I wrote an extremely tiny (< 50 LOC) API service in Node (I was playing around with Node a lot at the time). Since the entire premise of the ipify service is returning a string, I figured this was a perfect use case for Node as a technology: handling a lot of requests with minimal CPU usage.

After I had the API service built in Node, I threw together a simple static site to power the front-end and deployed it into an S3 bucket on Amazon. I then configured a CloudFront origin (Amazon’s CDN service) to sit in front of the S3 bucket and cache the pages for ultra-fast load times.

Now, I’m not a designer by any stretch of the imagination… But, with a little bootstrap love things turned out half-decent =)

After getting everything working I did some quick testing and things seemed to be working ok so I moved onto the next phase: deployment.

* ipify의 첫 코드는 상당히 단순했습니다. 50줄도 되지 않는 작은 코드를 Node로 작성했습니다. (당시에 많은 시간을 Node를 갖고 놀며 지냈습니다.) ipify 서비스가 문자열을 반환하는 일이 전부라는 전제에서는 Node를 사용할 가장 적합한 경우인 것을 알 수 있었습니다. 적은 CPU 사용량으로 수많은 요청을 처리하는 작업이죠.

API 서비스를 Node로 만들고 나서 간단한 정적 사이트를 프론트엔드로 붙여 아마존의 S3에 올렸습니다. 그리고 S3 버킷 앞에 아마존의 CDN 서비스인 CloudFront를 설정해서 페이지를 캐싱하는 것으로 엄청 빠르게 불러올 수 있도록 했습니다.

어떤 상상의 날개를 펼쳐봐도 저는 디자이너가 아니었습니다. 그래도 다행히 bootstrap을 사용해서 조금 나은 모습을 갖출 수 있었습니다. =)

모든 준비가 끝난 후에 간단하게 테스트를 수행했습니다. 모든 것이 준비되었다는 생각에 다음 단계인 배포로 넘어갔습니다.

## Enter Heroku

* I’m a big fan of Heroku (I’ve even written a book about it). I’ve been using it for many years now and consider it to be one of the most underrated services in the developer world. If you’ve never used it, go check it out!

I decided that if I wanted to run ipify in a scalable, highly available and cheap way then Heroku was the simplest and best option: so that’s what I went with.

I deployed ipify to Heroku in a minute or two, ran it on a single dyno (web server), and did some limited testing. Things again seemed to be working well and I was feeling pretty happy with myself.

If you aren’t familiar with Heroku, let me explain how ipify’s infrastructure was working:

Heroku ran my ipify web service on a small dyno (web server) with 512M RAM and limited CPU
If my process crashed or had any critical issues, Heroku would automatically restart it for me
Heroku ran a load balancer that accepted all incoming requests for my app and forwarded them to my dyno (web server) to process the request
This is a nice setup because:

Everything is highly available: Heroku’s load balancers, my dyno, everything
It requires no maintenance, configuration management, or any sort of deployment code. It’s 100% automated.
It’s cheap: I paid ~$7/mo to run this single web server
It’s fast: Heroku runs on top of Amazon Web Services (AWS), so my infrastructure was running in one of the most popular cloud hosted destinations in the world: AWS us-east (Virginia). This means it is geographically running on the east coast of the US: just across the water from Europe, and not terribly far from the rest of the continental US. This means users from most parts of the world wouldn’t have to make an enormous hop to reach the service.
At this point, I was feeling pretty good. This took < 1 day’s worth of work to build, setup, test, and move into production.

I then integrated ipify into my own infrastructure management code to solve the problem I had initially needed to resolve.

Things were going great for about a month before I started noticing some issues…

* 저는 Heroku의 왕 팬입니다. (심지어 Heroku에 대한 책도 썼습니다.) Heroku를 수년 간 사용하고 있고 개발 세계에 있어서 가장 저평가되고 있는 서비스 중 하나라고 생각합니다. 한 번도 사용해본 적이 없다면 지금 확인해보시길 바랍니다!

ipify를 확장 가능하고 고가용성으로, 그리고 저렴하게 운영하려면 Heroku 만큼 단순하고 좋은 선택지가 없다고 생각하고는 Heroku를 사용하기로 결정했습니다.

ipify를 Heroku에 1~2분 정도 걸려 배포하고 단일 dyno(웹서버)로 실행한 후에 제한적으로 테스트를 수행했습니다. 여전히 잘 동작하니 스스로 뿌듯한 기분이 들었습니다.

Heroku에 익숙하지 않다면 ipify 인프라스트럭처가 어떻게 동작하는지 살펴봅시다.

Heroku는 ipify 웹서비스를 512M 램과 제한된 CPU인 작은 dyno (웹서버)에서 구동합니다.
만약 프로세스가 충돌하거나 어떤 심각한 문제가 발생한다면 Heroku는 자동으로 서비스를 재시작합니다.
Heroku는 앱으로 들어오는 모든 요청을 로드 벨런스를 거친 후 요청을 처리하기 위해 dyno (웹서버)로 전달합니다.
이런 설정이 좋은 이유는:

고가용성: Heroku의 로드벨런서, 제 dyno, 모든 것이 고가용성입니다.
유지보수도, 관리 설정도, 어떤 배포 코드도 필요하질 않습니다. 100% 자동입니다.
저렴합니다. 단일 서버를 위해 월 ~7달러가량 냅니다.
빠릅니다. 헤로쿠는 아마존 웹서비스(AWS) 상에서 돌아가며 모든 인프라스트럭처는 세계에서 가장 인기있는 클라우드 호스트 위치인 AWS 미동부 (버지니아)에서 운영됩니다. 이 의미는 지리적으로 US 동부 해안에서 운영된다는 점인데 물 건너 유럽이나 나머지 US 지역에서 그다지 멀지 않다는 의미입니다. 세계 대부분 지역의 사용자가 서비스를 사용한다고 해도 그렇게 오래 걸리지는 않을 겁니다.
지금까지는 상당히 괜찮았습니다. 만들고, 설정하고, 테스트하고, 프로덕션으로 옮기기까지 1일 이하의 노력으로 끝낼 수 있었습니다.

그리고나서 ipify를 제 인프라스트럭처 관리 코드와 통합하는 것으로 원래 해결하려던 문제를 처리했습니다.

한 달 정도 모든 것이 잘 흘러갔습니다. 몇 가지 문제를 알아차리기 전까지는 말이죠.

## Popularity… Ugh

* I never marketed ipify, but it ended up ranking really high for the “ip address api” search phrase on Google. I guess all those years working on copy editing and SEO paid off.

Within a month or so, ipify was ranking near the top of Google search results, bringing in thousands of new users. With the increased visibility of the service, I started seeing some issues.

My Heroku load balancer was firing off warnings because my Node server wasn’t servicing incoming requests quickly enough. What ended up happening was:

Too many users would make API requests to ipify
My Node server would start responding to requests slowly so latency would rise
The Heroku load balancer would notice this and start buffering requests before sending them onto my Node server
Because my Node server wasn’t servicing requests quickly enough, my load balancer would return a 503 to the user and the request would die off
Not a pretty picture.

So what I did was simple: I added another Heroku dyno. This way, I’d have twice the capacity and things would run smoothly again. The downside is that running two “production” dynos on Heroku spiked my cost up to $50. Once you’ve gone past one dyno, you have to pay normal rates: $25/mo/dyno.

I figured that by now the service had likely capped out in popularity and I was OK paying $50/mo, so I’d just do that and things would go back to normal.

But… Things didn’t quite work out that way.

Before even a week had passed, I was already getting alerts from Heroku telling me that I was having the same issue as before. When I looked at my stats, I could see that my traffic had doubled, and again it appeared like ipify was just simply getting too much usage.

I added another dyno (brining my monthly cost up to ~$75/mo), but decided to investigate further. I’m a frugal guy – the idea of losing > $50/mo seemed unpleasant.

* ipify를 홍보한 적이 없지만 “ip address api”를 구글에서 검색하면 꽤 상단에 걸리게 되었습니다. SEO와 문구 수정에 사용한 수년 간의 노력이 빛을 발한 것 같습니다.

그동안 ipify가 구글 검색 결과에서 상당히 높은 순위로 노출된 덕분에 수천 명의 사용자가 생겼습니다. 서비스가 많이 노출되면서 몇 이슈가 나타나기 시작했습니다.

제 Heroku 로드 벨런서가 경고를 보내기 시작했습니다. Node 서버가 들어오는 요청을 충분히 빠르게 처리하지 못하고 있다는 내용이었습니다. 결과적으로 다음과 같은 일이 나타났습니다.

너무 많은 사용자가 ipify에 API 요청을 보내고 있습니다
제 Node 서버가 요청에 대해 느리게 응답해서 지연이 늘고 있었습니다
Heroku의 로드벨런서가 이 문제를 알아차리고 Node 서버에 보내기 전에 요청을 버퍼에 저장했다가 보내기 시작했습니다.
Node 서버가 빠르게 요청을 처리하지 못한 탓에 로드벨런서에서 사용자에게 503을 전달하고 요청을 끝내버렸습니다.
그다지 좋은 모습이 아닙니다.

이 문제를 해결하기 위해 한 일은 간단했습니다. Heroku에 dyno를 더 추가했습니다. 이 방식으로 수용량을 두 배로 늘렸고 모든 서비스가 부드럽게 동작하기 시작했습니다. 그 이면엔 두 “프로덕션” dyno를 구동한 탓에 거의 50달러 가량을 지불해야 했습니다. 첫 dyno 이후엔 일반 요금인 dyno당 월 25달러를 지불해야 했습니다.

서비스가 이 정도로 인기 있다면 월 50달러를 내는 것도 나쁘지 않다고 생각해서 지불하기로 결정하고 다시 안정적인 상황으로 돌아올 수 있었습니다.

하지만… 그렇게 말처럼 쉬운 문제가 아니었습니다.

한주도 채 지나기 전에 Heroku에서 이전과 동일한 경고를 받았습니다. 사용량을 보니 트래픽은 두 배로 늘었고 ipify는 더 많은 사용량을 보였습니다.

저는 또 다른 dyno를 추가하고 월 75달러를 지불하게 되었습니다. 그래도 좀 더 살펴보기로 했습니다. 저는 짠돌이라서 월 50달러 이상 쓰는 건 기쁜 일이 아니었습니다.



## The Investigation

When I started investigating what was going on, the first thing I did was check to see how many requests per second (rps) ipify was actually getting. I was pretty surprised by the number: it was low.

If my memory serves me correctly, ipify was only getting around 10 rps at the time. When I saw how small the number was, I figured something bad must be happening in my code. If I can’t service 10 rps across two small web servers, I must be doing something horribly wrong.

The first thing I noticed was that I was running a single Node process. This was an easy fix: I started using the Node cluster module, and bam: I was immediately running one process for each CPU core. This effectively doubled my throughput on my Heroku dynos.

But 20 rps still seemed like a tiny number so I did some more digging. Instead of doing load testing on Heroku, I did some load testing locally on my laptop.

My laptop was far stronger than a small 512M RAM Heroku dyno, so I figured I should see much better throughput.

I did some testing using the ab tool and was surprised to see that even on my laptop I was unable to surpass a threshold of 30 rps from my Node processes (I run Linux on my laptop and ab works effectively there). I then did some basic profiling and found that Node was spending a lot of time performing basic string manipulation operations (to extract the IP address from the X-Forwarded-For header and clean it up). No matter what I experimented with, I was unable to boost throughput up much above that limit.

At this point, the production ipify service was able to serve roughly 20 RPS across two dynos. ipify was in total serving ~52 million requests/mo. Not impressive.

I decided to rewrite the service in Go (which I started using a few months before) for performance purposes to see whether or not I could get more throughput out of an equivalent Go server.

ipify v1

Warrior Sketch

Rewriting ipify in Go was a short (but fun) experiment.

It gave me the opportunity to mess around with many different Go routing stacks: Gorilla/mux, Martini, and httprouter. After benchmarking and playing around with all three routing tools, I ended up using httprouter as it performed significantly better than the other two more popular options.

On my laptop I was able to achieve ~2,500 requests/second from my Go server. A massive improvement. The memory footprint was also much lower and hovered around 5M.

With my new found love for Go, I immediately took action and deployed my new Go-based ipify service on Heroku.

The results were fantastic. I was able to get about ~2k RPS of throughput from a single dyno! This brought my hosting back down to $25/mo and my total throughput to ~5.2 billion requests/mo.

Several days later, while talking to a more experienced Go developer, I ended up rewriting some of my string handling functionality which netted me an extra ~1k RPS of throughput. I At this point I was able to sustain ~7.7 billion requests/mo per dyno (give or take a bit).

I was thrilled to say the least.

More Popularity

Tyrael Sketch

Although I was able to cut back my hosting cost down to a reasonable range for a short period of time, within roughly two months ipify started experiencing issues once more.

It’s growth had continued to rise at an astounding rate. Around this time I had set up Google Alerts for ipify, so that I’d know when people mentioned it.

I started getting notifications that more and more people started using ipify in their personal and work projects. I then began receiving email from companies asking if they can embed it in their products (including a large smart TV provider, numerous media agencies, IoT vendors, etc.).

Before I knew it ipify was servicing around 15 billion requests/mo and I was now back to spending $50/mo on hosting costs.

But that too didn’t last for long.

I noticed rather quickly that ipify’s traffic continued to rapidly increase. There was a period of several months where it would add an additional billion requests each month.

I also started having issues with burst traffic – ipify would receive massive amounts of burst traffic for a short period of time that would die off quickly. I presumed this traffic was part of bootstrapping scripts, cron jobs, and other similar timed operations.

I later discovered through user notifications that antivirus vendors started blocking ipify because it started getting used in root kits, viruses, and other nasty pieces of software. Attackers would use ipify to get the victim’s public IP address before firing it off to a central location to be used for malicious purposes. I suppose this sort of usage was responsible for a large amount of that burst traffic.

While I’m not a fan of helping VX authors or spending money to assist them, I made the decision to keep ipify running neutrally to serve anyone who wants to use it. I’ve never been a fan of developer services that pick and choose what people use them for. I like to keep things simple.

Which left me to deal with the burst traffic problem. Dealing with burst traffic was tricky because I primarily had two choices:

Run additional dynos at a cost to myself and be always prepared for burst traffic, or
Use an auto scaling tool like Adept to automatically create and destroy dynos based on traffic patterns on my behalf
I eventually decided to manually involve myself and go with option #1, simply because I didn’t want to spend any additional funds on Adept’s service (even though I’ve used it before, and it is fantastic). Around this time I was spending $150/mo and ipify was servicing ~25 billion requests/mo.

Which brings us to the recent past.

ipify Hits 30 Billion Requests

Buzz Lightyear Proud Sketch

Over the past few months, ipify has hit a new record and surpassed 30 billion requests/mo on several occasions. It was an exciting milestone to surpass and something that has been fun to watch.

Today, ipify routinely serves between 2k and 20k RPS (which is almost never consistent). Traffic is always variable and the usage is so routinely high that I’ve completely given up on trying to detect traffic patterns in any meaningful way. Average response time ranges between 1 and 20ms depending on traffic patterns.

Today the service runs for between $150/mo and $200/mo depending on burst traffic and other factors. If I factor that into my calculator (assuming a $200/mo spend), it looks as if ipify is able to service each request for total cost of $0.000000007. That’s astoundingly low.

If you compare that to the expected cost of running the same service on something like Lambda, ipify would rack up a total bill of: $1,243.582/mo (compute) + $6,000/mo (requests) = ~$7,243.58. As a quick note, this is some back-of-a-napkin math. I plugged ipify’s numbers into a Lambda pricing example from the AWS website here: https://aws.amazon.com/lambda/pricing/.

All in all, I’m extremely satisfied with ipify’s cost. It’s a frugal service that serves a simple purpose.

ipify’s Future

Which leads me to the future. As ipify continues to grow, the service has gotten requests for a lot of different things: ipv6 support (something that Heroku does not support :(), better web design, other metadata about IP addresses, etc.

As I’m incredibly busy with other projects nowadays, I ended up passing ownership of ipify along to my good friends at https://www.whoisxmlapi.com.

Jonathan and team are good people working to build a portfolio of valuable and interesting developer API services.

While I still help out with things from time-to-time, Jonathan and team are currently implementing new features for ipify, and working hard to roll out some pretty cool changes that I’m excited about (including an improved UI and more data endpoint).

I look forward to seeing how ipify continues to grow over the coming years.

If you have any questions or would like to get in touch with me, please shoot me an email.

-R
